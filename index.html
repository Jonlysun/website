<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuqi Sun</title>
  
  <meta name="author" content="Yuqi Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <font size="6">Yuqi Sun (Â≠ôÁéâÈΩê)</font>
              </p>
              <p>Hi! My name is Yuqi Sun. I am currently a PhD candidate in School of Computer Science, Fudan University and under the supervision of Dr.Bo Yan. I received my B.Sc also from Fudan University in 2020. 
              </p>
              <p>
                My research interests are in computer vision, computer graphics, medical healthcare and machine learning. Specifically, I focus on novel view synthesis, 3D reconstruction, 2D/3D generative models. Recently I've been working on expanding the application of NeRF and Gaussian Splatting. I am also doing some research in AI for medical.
              </p>
              <p style="text-align:center">
                <a href="yqsun22@m.fudan.edu.cn">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=HDNkQkgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jonlysun">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="myimages/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="myimages/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>                                      
              <p>                
                2024-07: Two paper are accepted for ACM MM24 <br />
                2023-09: One paper is accepted for AAAI-24 <br />                     
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>                                      
              <p>                        
                I represent some of my publication here, and more works are ongoing. (*Equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Audio-Face.png" alt="prl" width="160" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle>Audio-Driven Identity Manipulation for Face Inpainting</papertitle>
              </a>
              <br>
              <strong>Yuqi Sun*</strong>, Qing Lin*, Weimin Tan, Bo Yan
              <br>
              <em>ACM MM</em>, 2024
              <br>
              <!-- <a href="">Project Page</a>
              /
              <a href="">Code</a>
              / -->
              <a href="https://openreview.net/forum?id=XY8iqCpOBF&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2024%2FConference%2FAuthors%23your-submissions)">Paper</a>
              <p>  Our main insight is that a person's voice carries distinct identity markers, such as age and gender, 
                which provide an essential supplement for identity-aware face inpainting. By extracting identity information from audio as guidance, 
                our method can naturally support tasks of identity preservation and identity swapping in face inpainting. 
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Data-Effect-Learning.png" alt="prl" width="160" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle>A Medical Data-Effective Learning Benchmark for Highly
                  Efficient Pre-training of Foundation Models</papertitle>
              </a>
              <br>
              Wenxuan Yang, Weimin Tan, <strong>Yuqi Sun</strong>, Bo Yan
              <br>
              <em>ACM MM</em>, 2024
              <br>
              <!-- <a href="">Project Page</a>
              /
              <a href="">Code</a>
              / -->
              <a href="https://openreview.net/forum?id=n10Ax1bixC&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2024%2FConference%2FAuthors%23your-submissions)">Paper</a>
              <p> This paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes
                a dataset with millions of data samples from 31 medical centers
                (DataDEL), a baseline method for comparison (MedDEL), and a new
                evaluation metric (NormDEL) to objectively measure data-effective
                learning performance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/STSS.png" alt="prl" width="160" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle>Low-Latency Space-Time Supersampling for Real-Time Rendering</papertitle>
              </a>
              <br>
              Ruian He*, Shili Zhou*, <strong>Yuqi Sun</strong>, Ri Cheng, Weimin Tan, Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <br>

              <a href="https://github.com/ryanhe312/STSSNet-AAAI2024">Code</a>
              /
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27982">Paper</a>
              <p>  We recognize the shared context and mechanisms between frame supersampling and extrapolation, 
                and present a novel framework, Space-time Supersampling (STSS). By integrating them into a unified framework, 
                STSS can improve the overall quality with lower latency. Notably, the performance is 
                achieved within only 4ms, saving up to 75% of time against the conventional two-stage pipeline that necessitates 17ms.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/instruct-neuralTalker.png" alt="prl" width="160" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle>Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with
                  Instructions</papertitle>
              </a>
              <br>
              <strong>Yuqi Sun</strong>, Reian He, Weimin Tan, Bo Yan
              <br>
              <em>Arxiv</em>, 2023
              <br>           
              <a href="https://arxiv.org/abs/2306.10813">Paper</a>
              <p>          We propose Instruct-NeuralTalker, the first interactive framework
                to semantically edit the audio-driven talking radiance fields
                with simple human instructions. It supports various taking face editing
                tasks, including instruction-based editing, novel view synthesis,
                and background replacement. In addition, Instruct-NeuralTalker
                enables real-time rendering on consumer hardware.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Resolution.png" alt="prl" width="145" height="145">
            </td>
            <td width="75%" valign="middle">             
                <papertitle>Geometry-Aware Reference Synthesis for Multi-View Image
                  Super-Resolution</papertitle>
              </a>
              <br>
              Ri Cheng, <strong>Yuqi Sun</strong>, Bo Yan, Weimin Tan, Chenxi Ma
              <br>
              <em>ACM MultiMedia</em>, 2022
              <br>
              <a href="https://github.com/Orange066/MVSR">Code</a>
              /
              <a href="https://arxiv.org/pdf/2207.08601.pdf">Paper</a>
              <p>This paper proposes a Multi-View Image SuperResolution (MVISR) task. It aims to increase the resolution of multiview images captured from the same scene. One solution is to apply
                image or video super-resolution (SR) methods to reconstruct HR
                results from the low-resolution (LR) input view. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/SIBRNet.png" alt="blind-date" width="160" height="100">
            </td>
            <td width="75%" valign="middle">              
                <papertitle>Learning Robust Image-Based Rendering on Sparse Scene Geometry
                  via Depth Completion</papertitle>    
              <br>
              <strong>Yuqi Sun</strong>, Shili Zhou, Ri Cheng, Weimin Tan, Bo Yan*, Lang Fu
              <br>              
              <em>CVPR</em>, 2022
              <br>
              <a href="https://github.com/Jonlysun/SIBRNet">Code</a>
              /
              <a href="https://www.bilibili.com/video/BV1Lr4y1q7Yt/">Video</a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Learning_Robust_Image-Based_Rendering_on_Sparse_Scene_Geometry_via_Depth_CVPR_2022_paper.pdf">Paper</a>
              <p>Recent image-based rendering (IBR) methods usually
                adopt plenty of views to reconstruct dense scene geometry.
                However, the number of available views is limited in practice.
                When only few views are provided, the performance
                of these methods drops off significantly, as the scene geometry
                becomes sparse as well. Therefore, in this paper, we
                propose Sparse-IBRNet (SIBRNet) to perform robust IBR
                on sparse scene geometry by depth completion.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/SASRNet.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">          
                <papertitle>Space-Angle Super-Resolution for Multi-View Images</papertitle>              
              <br>
              <strong>Yuqi Sun*</strong>, Ri Cheng*, Bo Yan, Shili Zhou
              <br>
              <em>ACM MultiMedia</em>, 2021
              <br>
              <a href="https://github.com/Jonlysun/SASRNet">Code</a>
              /
              <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475244">Paper</a>
              <p>The limited spatial and angular resolutions in multi-view multimedia
                applications restrict their visual experience in practical use. In
                this paper, we first argue the space-angle super-resolution (SASR)
                problem for irregular arranged multi-view images. It aims to increase
                the spatial resolution of source views and synthesize arbitrary
                virtual high resolution (HR) views between them jointly.</p>
          
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
              <p>
                Some internship experiences
              </p>
            </td>      
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
				
                    
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="myimages/sensetime.png" alt="cs188">
            </td>
            <td width="75%" valign="middle"> 
              <a href="https://www.sensetime.com/cn">
                 <papertitle>Autonomous Driving Division, sensetime</papertitle>
              </a>         
              <br>                
              <span id="right">May 2019 - 2020</span>
              <br>
              <p>
                <ul>
                  <li>Dynamic scene sensing</li>                  
                  <li>Simulator data processing and driving trajectory prediction</li>                  
                </ul>    
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle"><img src="myimages/fuhuan.png", height="160", width="160"></td>
            <td width="75%" valign="middle">
              <a href="https://www.mewhooo.cn/#/index">
                <papertitle> Shanghai Fuhuan Science and Technology</papertitle>
              </a>
              <br>                
              <span id="right">May 2019 - 2020</span>
              <br>
              <p>
                <ul>
                  <li>Research on face skin health detection algorithm</li>                  
                </ul>
    
              </p>
            </td>            
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Social</heading>
              <p>
                I will upload some personal videos and articles on my social media.                
                <p>
                  <ul>
                    <li>Build Pre-Training Model (in medical) <a href="https://github.com/Jonlysun/website/blob/master/myarticles/PreTrainingModel.pdf">Github</a></li>                  
                    <li>AIGC-3D <a href="https://www.bilibili.com/video/BV19h4y1M7aW/?spm_id_from=333.999.0.0">Bilibili</a></li>                  
                    <li>NeRFÂèäÂÖ∂ÂèëÂ±ï <a href="https://zhuanlan.zhihu.com/p/512538748">Zhihu</a></li>                                      
                  </ul>                                                   
                </p>
                
              </p>
            </td>      
          </tr>
        </tbody></table>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">Yuqi Sun</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
