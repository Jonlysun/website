<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuqi Sun</title>
  
  <meta name="author" content="Yuqi Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <font size="6">Yuqi Sun (Â≠ôÁéâÈΩê)</font>
              </p>
              <p>Hi! My name is Yuqi Sun. I got my PhD degree in 2025 from the School of Computer Science at Fudan University, under the supervision of Dr.Bo Yan. I also received my B.Sc. from Fudan University in 2020. 
              </p>
              <p>
                My research interest lies in leveraging artificial intelligence techniques for data governance (<b>AI for data</b>)‚Äîincluding, 
                but not limited to, <b>data management</b>, <b>data filtering</b>, and <b>data synthesis</b>‚Äîto establish a data foundation for 
                building low-cost, high-performing AI models. My previous research focused on multi-view imaging, face images, 
                and rendered images, with a recent shift toward scientific fields such as medical imaging. I strongly believe 
                that data governance is one of the most critical directions for AI innovation, essential for reducing model 
                training costs, and I aim to extend its application to more scientific domains in the future.
              </p>
              <p>                
                After graduation, I co-founded FlyAiTech, a company dedicated to providing data governance and large-model product services to 
                hospitals, public security agencies, manufacturing enterprises, and other organizations.

              </p>

              <p style="text-align:center">
                <a href="yuqisunfd@163.com">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=HDNkQkgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jonlysun">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="myimages/me3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="myimages/me3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>                                      
              <p>
                2025-05: Two paper were accepted for ACM MM25 <br />            
                2025-03: Our new work has been published in <b>Nature Biomedical Engineering (IF: 28.0)</b>!  <br />                
                2024-07: Two paper were accepted for ACM MM24 <br />                         
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards and Honors</heading>                                      
              <p>
                <ul>
                  <li>Graduate Representative Speaker, Fudan University Graduate Commencement Ceremony, 2025</li>
                  <li>Fudan Academic Star Special Award, Fudan University, 2025 (Awarded to only 6 students across the entire university)</li>
                  <li>Outstanding Graduate, Fudan University, 2025</li>
                  <li>Fudan Top 10 Scientific Advances Nominee, Fudan University, 2024</li>
                  <li>Shanghai Yangpu 'Entrepreneurship Star' Emerging Talent Award, 2024</li>
                  <li>National Disruptive Technology Innovation Competition Winner, 2024</li>
                  <li>Featured in Media: Research covered by CCTV News and CCTV Defense and Military Channel, 2024</li>
                </ul>                       
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>                                      
              <p>                        
                I represent some of my publication here, and more works are ongoing. (*Equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/MMSkin.png" alt="prl" width="200" height="150">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks</strong></papertitle>
              </a>
              <br>
              Wenqi Zeng, <strong>Yuqi Sun</strong>, Chenxi Ma, Weimin Tan, Bo Yan
              <br>
              <em>ACM MM</em>, 2025-05
              <br>
              <!-- <a href="">Project Page</a>             -->
              <a href="https://github.com/ZwQ803/MM-Skin">Code</a>
              /
              <a href="https://arxiv.org/abs/2505.06152">Paper</a>
              <p> Medical vision-language models (VLMs) show potential as clinical assistants, but dermatology-specific VLMs 
                lack detailed diagnostic capabilities due to limited dataset text descriptions. We introduce MM-Skin, a large-scale 
                multimodal dermatology dataset with 3 imaging modalities (clinical, dermoscopic, pathological) and ~10,000 high-quality
                 image-text pairs from textbooks, plus ~27,000 diverse VQA samples. Using MM-Skin and public datasets, we developed SkinVL,
                  a specialized VLM for accurate skin disease interpretation, advancing clinical dermatology VLM development.

                </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/TabMed.png" alt="prl" width="200" height="150">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>TabiMed: Tabularizing Medical Images for Few-Shot In-Context Diagnosis</strong></papertitle>
              </a>
              <br>
              Wanying Zhou, <strong>Yuqi Sun</strong>, Yu Ling, Zhen Xing, Chenxi Ma, Weimin Tan, Bo Yan
              <br>
              <em>ACM MM</em>, 2025-05
              <!-- <br> -->
              <!-- <a href="">Project Page</a> -->            
              <!-- <a href="https://github.com/Jonlysun/DERETFound">Code</a> -->
              <!-- / -->
              <!-- <a href="https://www.nature.com/articles/s41551-025-01365-0">Paper</a> -->
              <p> In biomedical image AI, TabiMed is a novel framework that solves the small-sample problem by transforming visual 
                representations into structured tabular data. Unlike slow and overfitting-prone supervised fine-tuning (SFT) or 
                inefficient zero-shot inference, TabiMed leverages in-context learning (ICL) and pre-trained tabular models to achieve
                 superior accuracy and efficiency, with an average AUC that is 14.1% higher than zero-shot and a training speed 250 times
                  faster than SFT. This approach offers a new and effective way to analyze biomedical images with limited data.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/RETFound-DE.png" alt="prl" width="200" height="150">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>A data-efficient strategy for building high-performing medical foundation models</strong></papertitle>
              </a>
              <br>
              <strong>Yuqi Sun*</strong>, Weimin Tan*, Zhuoyao Gu, Ruian He, Siyuan Chen, Miao Pang, Bo Yan
              <br>
              <em>Nature Biomedical Engineering</em>, 2025-03-05
              <br>
              <!-- <a href="">Project Page</a> -->            
              <a href="https://github.com/Jonlysun/DERETFound">Code</a>
              /
              <a href="https://www.nature.com/articles/s41551-025-01365-0">Paper</a>
              <p> Medical Foundation models typically require massive datasets, but medical data collection is costly, slow, and privacy-sensitive. 
                We demonstrate that synthetic data, generated with disease labels, can effectively pretrain medical foundation models. 
                Our retinal model, pretrained on one million synthetic retinal images and just 16.7% of the real-world data used by RETFound (904,170 images), 
                matches or exceeds RETFound‚Äôs performance across nine public datasets and four diagnostic tasks. We also validate this data-efficient approach by building 
                a tuberculosis classifier on chest X-rays. Text-conditioned synthetic data boosts medical model performance and generalizability with less real data.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Audio-Face.png" alt="prl" width="200" height="130">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>Audio-Driven Identity Manipulation for Face Inpainting</strong></papertitle>
              </a>
              <br>
              <strong>Yuqi Sun*</strong>, Qing Lin*, Weimin Tan, Bo Yan
              <br>
              <em>ACM MM</em>, 2024
              <br>
              <!-- <a href="">Project Page</a> -->
              
              <a href="https://github.com/Jonlysun/AudioFace">Code</a>
              /
              <a href="https://openreview.net/forum?id=XY8iqCpOBF&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2024%2FConference%2FAuthors%23your-submissions)">Paper</a>
              <p>  Our main insight is that a person's voice carries distinct identity markers, such as age and gender, 
                which provide an essential supplement for identity-aware face inpainting. By extracting identity information from audio as guidance, 
                our method can naturally support tasks of identity preservation and identity swapping in face inpainting. 
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Data-Effect-Learning.png" alt="prl" width="200" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>A Medical Data-Effective Learning Benchmark for Highly
                  Efficient Pre-training of Foundation Models</strong></papertitle>
              </a>
              <br>
              Wenxuan Yang, Weimin Tan, <strong>Yuqi Sun</strong>, Bo Yan
              <br>
              <em>ACM MM</em>, 2024
              <br>
              <!-- <a href="">Project Page</a>
              /
              <a href="">Code</a>
              / -->
              <a href="https://openreview.net/forum?id=n10Ax1bixC&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2024%2FConference%2FAuthors%23your-submissions)">Paper</a>
              <p> This paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes
                a dataset with millions of data samples from 31 medical centers
                (DataDEL), a baseline method for comparison (MedDEL), and a new
                evaluation metric (NormDEL) to objectively measure data-effective
                learning performance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/STSS.png" alt="prl" width="200" height="160">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>Low-Latency Space-Time Supersampling for Real-Time Rendering</strong></papertitle>
              </a>
              <br>
              Ruian He*, Shili Zhou*, <strong>Yuqi Sun</strong>, Ri Cheng, Weimin Tan, Bo Yan
              <br>
              <em>AAAI</em>, 2024
              <br>

              <a href="https://github.com/ryanhe312/STSSNet-AAAI2024">Code</a>
              /
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27982">Paper</a>
              <p>  We recognize the shared context and mechanisms between frame supersampling and extrapolation, 
                and present a novel framework, Space-time Supersampling (STSS). By integrating them into a unified framework, 
                STSS can improve the overall quality with lower latency. Notably, the performance is 
                achieved within only 4ms, saving up to 75% of time against the conventional two-stage pipeline that necessitates 17ms.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/instruct-neuralTalker.png" alt="prl" width="200" height="180">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with
                  Instructions</strong></papertitle>
              </a>
              <br>
              <strong>Yuqi Sun</strong>, Reian He, Weimin Tan, Bo Yan
              <br>
              <em>Arxiv</em>, 2023
              <br>           
              <a href="https://arxiv.org/abs/2306.10813">Paper</a>
              <p>          We propose Instruct-NeuralTalker, the first interactive framework
                to semantically edit the audio-driven talking radiance fields
                with simple human instructions. It supports various taking face editing
                tasks, including instruction-based editing, novel view synthesis,
                and background replacement. In addition, Instruct-NeuralTalker
                enables real-time rendering on consumer hardware.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/Resolution.png" alt="prl" width="200" height="145">
            </td>
            <td width="75%" valign="middle">             
                <papertitle><strong>Geometry-Aware Reference Synthesis for Multi-View Image
                  Super-Resolution</strong></papertitle>
              </a>
              <br>
              Ri Cheng, <strong>Yuqi Sun</strong>, Bo Yan, Weimin Tan, Chenxi Ma
              <br>
              <em>ACM MultiMedia</em>, 2022
              <br>
              <a href="https://github.com/Orange066/MVSR">Code</a>
              /
              <a href="https://arxiv.org/pdf/2207.08601.pdf">Paper</a>
              <p>This paper proposes a Multi-View Image SuperResolution (MVISR) task. It aims to increase the resolution of multiview images captured from the same scene. One solution is to apply
                image or video super-resolution (SR) methods to reconstruct HR
                results from the low-resolution (LR) input view. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/SIBRNet.png" alt="blind-date" width="200" height="130">
            </td>
            <td width="75%" valign="middle">              
                <papertitle><strong>Learning Robust Image-Based Rendering on Sparse Scene Geometry
                  via Depth Completion</strong></papertitle>    
              <br>
              <strong>Yuqi Sun</strong>, Shili Zhou, Ri Cheng, Weimin Tan, Bo Yan*, Lang Fu
              <br>              
              <em>CVPR</em>, 2022
              <br>
              <a href="https://github.com/Jonlysun/SIBRNet">Code</a>
              /
              <a href="https://www.bilibili.com/video/BV1Lr4y1q7Yt/">Video</a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Learning_Robust_Image-Based_Rendering_on_Sparse_Scene_Geometry_via_Depth_CVPR_2022_paper.pdf">Paper</a>
              <p>Recent image-based rendering (IBR) methods usually
                adopt plenty of views to reconstruct dense scene geometry.
                However, the number of available views is limited in practice.
                When only few views are provided, the performance
                of these methods drops off significantly, as the scene geometry
                becomes sparse as well. Therefore, in this paper, we
                propose Sparse-IBRNet (SIBRNet) to perform robust IBR
                on sparse scene geometry by depth completion.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="myimages/SASRNet.png" alt="clean-usnob" width="200" height="200">
            </td>
            <td width="75%" valign="middle">          
                <papertitle><strong>Space-Angle Super-Resolution for Multi-View Images</strong></papertitle>              
              <br>
              <strong>Yuqi Sun*</strong>, Ri Cheng*, Bo Yan, Shili Zhou
              <br>
              <em>ACM MultiMedia</em>, 2021
              <br>
              <a href="https://github.com/Jonlysun/SASRNet">Code</a>
              /
              <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475244">Paper</a>
              <p>The limited spatial and angular resolutions in multi-view multimedia
                applications restrict their visual experience in practical use. In
                this paper, we first argue the space-angle super-resolution (SASR)
                problem for irregular arranged multi-view images. It aims to increase
                the spatial resolution of source views and synthesize arbitrary
                virtual high resolution (HR) views between them jointly.</p>
          
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
              <p>
                Some internship experiences
              </p>
            </td>      
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
				
                    
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="myimages/sensetime.png" alt="cs188">
            </td>
            <td width="75%" valign="middle"> 
              <a href="https://www.sensetime.com/cn">
                 <papertitle>Autonomous Driving Division, sensetime</papertitle>
              </a>         
              <br>                
              <span id="right">May 2019 - 2020</span>
              <br>
              <p>
                <ul>
                  <li>Dynamic scene sensing</li>                  
                  <li>Simulator data processing and driving trajectory prediction</li>                  
                </ul>    
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle"><img src="myimages/fuhuan.png", height="160", width="160"></td>
            <td width="75%" valign="middle">
              <a href="https://www.mewhooo.cn/#/index">
                <papertitle> Shanghai Fuhuan Science and Technology</papertitle>
              </a>
              <br>                
              <span id="right">May 2019 - 2020</span>
              <br>
              <p>
                <ul>
                  <li>Research on face skin health detection algorithm</li>                  
                </ul>
    
              </p>
            </td>            
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Social</heading>
              <p>
                I will upload some personal photography, videos and articles on my social media.                
                <p>
                  <ul>
                    <li>Personal Photography <a href="https://www.xiaohongshu.com/user/profile/5f9c2c27000000000100734a">Rednote</a></li>                                      
                    <li>DeepSeek-R1Â∫îÁî® <a href="https://github.com/Jonlysun/website/blob/master/myarticles/DeepSeek-R1Â∫îÁî®.pdf">Github</a></li>        
                    <li>Build Pre-Training Model (in medical) <a href="https://github.com/Jonlysun/website/blob/master/myarticles/PreTrainingModel.pdf">Github</a></li>                  
                    <li>AIGC-3D <a href="https://www.bilibili.com/video/BV19h4y1M7aW/?spm_id_from=333.999.0.0">Bilibili</a></li>                  
                    <li>NeRFÂèäÂÖ∂ÂèëÂ±ï <a href="https://zhuanlan.zhihu.com/p/512538748">Zhihu</a></li>                                      
                  </ul>                                                   
                </p>
                
              </p>
            </td>      
          </tr>
        </tbody></table>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">Yuqi Sun</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
